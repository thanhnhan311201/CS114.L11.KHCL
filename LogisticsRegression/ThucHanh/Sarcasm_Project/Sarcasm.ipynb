{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sarcasm.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOFghMnYslSK9GgNRBIhz8x",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/thanhnhan311201/CS114.L11.KHCL/blob/master/Sarcasm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pFvGnFIUcfLr"
      },
      "source": [
        "# Code : Nguyễn Thành Nghĩa\n",
        "# Mô tả bài toán: Phân loại tiêu đề mang tính chất trào phúng. \n",
        "- Dữ liệu đầu vào: là các dòng tiêu đề có tính chất trào phúng và những tiêu đề không có tính chất trào phúng.\n",
        "- Yêu cầu: cho một bộ những tiêu đề chưa được phân loại, hãy xác định xem chúng thuộc loại có tính chất trào phúng hay không.\n",
        "\n",
        "# Ý tưởng\n",
        "- Lọc ra những từ có tác động đáng kể đến dataset \n",
        "- Đưa các dòng tiêu đề của dữ liệu đầu vào về dạng túi từ (bag-of-words)\n",
        "- Dùng mô hình Multinominal Naive Bayes với dữ liệu vừa xử lý để dự đoán kết quả\n",
        "\n",
        "# Link dataset: https://www.kaggle.com/rmisra/news-headlines-dataset-for-sarcasm-detection\n",
        "@article{misra2019sarcasm,\n",
        "  title={Sarcasm Detection using Hybrid Neural Network},\n",
        "  author={Misra, Rishabh and Arora, Prahal},\n",
        "  journal={arXiv preprint arXiv:1908.07414},\n",
        "  year={2019}\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a6q73KwodU-f"
      },
      "source": [
        "# Clone từ github để lấy dữ liệu\n",
        "!git clone https://github.com/thanhnhan311201/CS114.L11.KHCL.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RI3Y8w-fJC4T"
      },
      "source": [
        "!pip install stop-words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mTsvGxcFuzp7"
      },
      "source": [
        "# Thêm một số thư viện cần thiết\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from collections import deque \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A7_UGt3-vsoL"
      },
      "source": [
        "#Load dataset\n",
        "file_ = pd.read_json(\"/content/CS114.L11.KHCL/LogisticsRegression/Sarcasm_Project/Sarcasm_Headlines_Dataset.json\",lines=True)\n",
        "file_.head()\n",
        "del file_['article_link']\n",
        "# Training data // dùng 20000 dòng của dataset làm training data\n",
        "data = file_.to_numpy()[:20000,:]\n",
        "# Testing data // khoảng 6000 dòng còn lại làm testing data\n",
        "data2 = file_.to_numpy()[20000:,:]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zZamI6eMyFHi"
      },
      "source": [
        "# Lọc lại văn bản, chỉ giữ lại chữ và số\n",
        "text = ''\n",
        "for i in range(len(data)):\n",
        "  text += data[i,0]\n",
        "text = deque(text)\n",
        "while i < len(text):\n",
        "  if not((text[i] >='a' and text[i] <='z') or (text[i] >='0' and text[i] <='9') or (text[i] == ' ')):\n",
        "    text.remove(text[i])\n",
        "    i-=1\n",
        "  i+=1\n",
        "text = ''.join(text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G3rLe2IP057y"
      },
      "source": [
        "# thống kê số lượng từ xuất hiện trong toàn bộ dataset\n",
        "text = deque(text.split())\n",
        "dic = {}\n",
        "for i in range(len(text)):\n",
        "  dic[text[i]] = text.count(text[i])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WU2i1clh0wzU"
      },
      "source": [
        "# download bộ những từ không có ý nghĩa nhiều trong tiếng anh\n",
        "nltk.download('stopwords')\n",
        "stop = set(stopwords.words('english'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jKA6h4ZEeOpF"
      },
      "source": [
        "\n",
        "# Lọc bỏ ra những từ xuất hiện ít trong dataset có thể gây nhiễu và chia thành dic thành 2 list tiện cho xử lý\n",
        "# số num có tính tương đối\n",
        "num = 10\n",
        "val = list(i for i in dic.values() if i > num)\n",
        "key = list(i for i in dic.keys() if dic[i] > num)\n",
        "\n",
        "# Tính độ lệch chuẩn tần suất xuất hiện của các từ\n",
        "std = np.std(val)\n",
        "mean = np.mean(val)\n",
        "\n",
        "count = 0\n",
        "filtered_words = []\n",
        "# Lọc ra những từ có ảnh hưởng tưởng đối đến dataset\n",
        "for i in range(len(val)):\n",
        "  if key[i] not in stop and val[i] >= abs(mean-std) and val[i] <= abs(mean+std):\n",
        "    filtered_words.append(key[i])\n",
        "    count +=1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bB0r8lmd3gJi"
      },
      "source": [
        "# hàm Chuyển 1 câu sang vectors từ (bag words)\n",
        "def Sentences2Vector(s,w):\n",
        "  v = []\n",
        "  for i in range(len(w)):\n",
        "    v.append(s.count(w[i]))\n",
        "  return v\n",
        "# chuyển training data sang dạng vector\n",
        "vectors = []\n",
        "for i in range(len(data)):\n",
        "  vectors.append(Sentences2Vector(data[i,0],filtered_words))\n",
        "vectors = np.array(vectors)\n",
        "\n",
        "\n",
        "v_sarcasm = []\n",
        "v_none_sarcasm = []\n",
        "# Phân loại traning data ra sarcasm và none sarcasm\n",
        "for i in range(len(vectors)):\n",
        "  label = data[i,-1]\n",
        "  t = vectors[i,:] \n",
        "  if label == 1:\n",
        "    v_sarcasm.append(t)\n",
        "  if label == 0:\n",
        "    v_none_sarcasm.append(t)\n",
        "\n",
        "# chuyển sang numpy\n",
        "v_sarcasm = np.array(v_sarcasm) \n",
        "v_none_sarcasm = np.array(v_none_sarcasm)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6GFfvr_y2smC"
      },
      "source": [
        "# Sử dụng Model Multinomial Naive Bayes để dự đoán\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1qIC7Lci2sFs"
      },
      "source": [
        "# Hàm Smooth\n",
        "def Smooth(v):\n",
        "  if 0 in v:\n",
        "    v = v + 1\n",
        "  v = v/sum(v)\n",
        "  return v"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GYGc-e_ODdhR"
      },
      "source": [
        "# Tính tổng các vector của 2 labels\n",
        "sum_v_sarcasm = np.sum(v_sarcasm,axis=0)\n",
        "sum_v_none_sarcasm = np.sum(v_none_sarcasm,axis=0)\n",
        "# Smooth lại kết quả\n",
        "smooth_v_sarcasm = Smooth(sum_v_sarcasm)\n",
        "smooth_v_none_sarcasm = Smooth(sum_v_none_sarcasm)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IAeFSKdhFk3y"
      },
      "source": [
        "# Chuyển Testing data sang dạng vector\n",
        "v_test = []\n",
        "for i in range(len(data2)):\n",
        "  v_test.append(Sentences2Vector(data2[i,0],filtered_words))\n",
        "v_test = np.array(v_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hQHnGD1sps8s",
        "outputId": "4339a404-d5fd-4a30-db2a-9eb3d0254bc9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "S = len(v_sarcasm)\n",
        "N = len(v_none_sarcasm)\n",
        "total = S + N\n",
        "result = []\n",
        "count = 0\n",
        "error = 0\n",
        "# Tính toán và dự đoán kết quả của từng testcase\n",
        "for i in v_test:\n",
        "  PS = S/total \n",
        "  PN = N/total\n",
        "  RS = PS * np.prod(pow(smooth_v_sarcasm,i)) # Xác suất tin sarcasm\n",
        "  RN = PN * np.prod(pow(smooth_v_none_sarcasm,i)) # Xác suất tin none sarcasm\n",
        "  if RS >= RN:\n",
        "    result.append(1)\n",
        "  else:\n",
        "    result.append(0)\n",
        "  # Kết quả dự đoán khác kết quả thực tế tăng error lên 1\n",
        "  if result[count] != data[count,1]:\n",
        "    error += 1\n",
        "  count += 1\n",
        "\n",
        "# In ra độ sai lệch\n",
        "print(\"Error = {}%\".format(round(error*100/len(data2),3)))\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Error = 45.074%\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}